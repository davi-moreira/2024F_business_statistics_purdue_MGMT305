---
title: "<span style = 'font-size: 100%;'> MGMT 30500: Business Statistics </span>"
subtitle: "<span style = 'font-size: 150%;'> Multiple Regression</span>"
author: "Professor<br>Davi Moreira<br>"
date: "2024-08-01"
date-format: "MMMM DD, YYYY"
format:
  revealjs: 
    transition: slide
    background-transition: fade
    width: 1600
    height: 900
    center: true
    slide-number: true
    incremental: true
    chalkboard: 
      buttons: false
    preview-links: auto
    #logo: images/quarto.png
    footer: "Business Statistics"
    theme: [simple,custom.scss]
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

## Overview

::: nonincremental
::: columns
::: {.column width="50%" style="text-align: center; justify-content: center; align-items: center;"}
- Multiple Regression Model
- Least Squares Method
- Multiple Coefficient of Determination
- Model Assumptions
- Testing for Significance: Overall and Individual
:::

::: {.column width="50%" style="text-align: center; justify-content: center; align-items: center;"}
- Multicollinearity Issue
- Residual Analysis
- Prediction
- Categorical Independent Variables
:::
:::
:::

# Multiple Regression {background-color="#cfb991"}

## Multiple Regression

- Regression analysis involving two or more independent variables (x's).

- This subject area, called multiple regression analysis, enables us to consider more independent variables (factors) and thus obtain better estimates of the relationship than are possible with simple linear regression.

## Multiple Regression Model

The equation that describes how the dependent variable $y$ is related to the independent variables $x_1, x_2, \ldots x_p$ and an error term $\epsilon$ is:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon
$$

Where:

- $\beta_0, \beta_1, \beta_2, \dots, \beta_p$ are the unknown parameters.

- $\epsilon$ is a random variable called the error term with the same assumptions as in simple regression (Normality, zero mean, constant variance, independence).

- $p$ is the number of independent variables (dimension or complexity of the model).


## Multiple Regression Equation

The equation that describes how the mean value of $y$ is related to $x_1, x_2, \ldots x_p$ is:

$$
E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
$$

- $\beta_1, \ldots, \beta_p$ measure the marginal effects of the respective independent variables.

For example, $\beta_1$ is the change in $E(y)$ corresponding to a 1-unit increase in $x_1$, when all other independent variables are held constant or when we control for all other independent variables.


## Estimated Multiple Regression Equation

$$
\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_p x_p
$$

- A simple random sample is used to compute sample slopes $b_0, b_1, b_2, \dots, b_p$ that are used as the point estimators of the population slopes $\beta_0, \beta_1, \beta_2, \dots, \beta_p$.

Hence, $\hat{y}$ estimates $E(Y)$.

# Least Squares Method {background-color="#cfb991"}

## Least Squares Method

- Least Squares Criterion: Minimize the Sum of Squared Errors (SSE):

$$
\min \sum (y_i - \hat{y_i})^2
$$

Where $y_i - \hat{y_i}$ is the $i$-th residual/error.

- The formulas for the regression coefficients $b_0, b_1, b_2, \dots, b_p$ involve the use of matrix algebra. We will rely on computer software packages to perform the calculations.

- The emphasis will be on how to interpret the computer output rather than on how to make the multiple regression computations.


## Estimation Process

XXX incluir figura XXX

## Multiple Regression Model 

**Example: Butler Trucking Company**

Managers at Butler Trucking Company want to develop better work schedules for their drivers. They believe that the total daily travel time would be closely related to the number of miles traveled in making the daily deliveries and also to the number of deliveries.

A simple random sample of 10 driving assignments was taken.

## Multiple Regression Model 

**Example: Butler Trucking Company**: `Butler.xlsx`

| Driving Assignment | Miles traveled $x_1$ | Deliveries $x_2$ | $y$ = Travel Time (hours) |
|--------------------|------------------------|--------------------|-----------------------------|
| 1                  | 100                    | 4                  | 9.3                         |
| 2                  | 50                     | 3                  | 4.8                         |
| 3                  | 100                    | 4                  | 8.9                         |
| 4                  | 100                    | 2                  | 6.5                         |
| 5                  | 50                     | 2                  | 4.2                         |
| 6                  | 80                     | 2                  | 6.2                         |
| 7                  | 75                     | 3                  | 7.4                         |
| 8                  | 65                     | 4                  | 6.0                         |
| 9                  | 90                     | 3                  | 7.6                         |
| 10                 | 90                     | 2                  | 6.1                         |

## Multiple Regression Model 

**Example: Butler Trucking Company**

Suppose we believe that total daily travel time ($y$) is related to the miles traveled ($x_1$) and the number of deliveries made ($x_2$) by the following multiple linear regression model:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon
$$

Where:
- $y$ = Total travel time
- $x_1$ = Miles traveled
- $x_2$ = Deliveries made
- $n = 10, p = 2$

## Summary Statistics

|                  | Miles ($x_1$) | Deliveries ($x_2$) | Time ($Y$) |
|------------------|-----------------|----------------------|--------------|
| Mean             | 80              | 2.9                  | 6.7          |
| Standard Error   | 6.191           | 0.277                | 0.515        |
| Median           | 85              | 3                    | 6.35         |
| Mode             | 100             | 2                    | #N/A         |
| Standard Deviation | 19.579         | 0.876                | 1.630        |
| Sample Variance  | 383.333         | 0.767                | 2.656        |
| Kurtosis         | -1.114          | -1.734               | -0.547       |
| Skewness         | -0.583          | 0.223                | 0.196        |
| Range            | 50              | 2                    | 5.1          |
| Minimum          | 50              | 2                    | 4.2          |
| Maximum          | 100             | 4                    | 9.3          |
| Sum              | 800             | 29                   | 67           |
| Count            | 10              | 10                   | 10           |


## Correlations

|                    | Miles (x1) | Deliveries (x2) | Time (Y) |
|--------------------|------------|-----------------|----------|
| **Miles (x1)**      | 1          |                 |          |
| **Deliveries (x2)** | 0.162      | 1               |          |
| **Time (Y)**        | 0.815      | 0.615           | 1        |

## Regression Output

**Example: Butler Trucking Company**

XXX include figure XXX

![Regression Output](file path or image URL)


## Estimated Regression Equation

**Example: Butler Trucking Company**

Prediction equation:

$$
\hat{y} = -0.8687 + 0.0611x_1 + 0.9234x_2 \quad \text{(See next page for interpretations.)}
$$

- For observation #1 ($x_1 = 100$, $x_2 = 4$, and $y = 9.3$):

$$
\hat{y} = -0.8687 + 0.0611(100) + 0.9234(4) = 8.9385
$$

Unexplained residual = $y - \hat{y} = 9.3 - 8.9385 = 0.3615$


## Interpreting the Regression Coefficients 

Each $b_i$ (for $x_i$) represents an estimate of the change in the expected $y$ corresponding to a 1-unit increase in $x_i$, when all other independent variables are held constant or when we control for all other independent variables.


## Interpreting the Regression Coefficients

**Example: Butler Trucking Company**

- $b_1 = +0.0611$ (for $x_1$)

  +0.0611 is the estimated change (increase) in the expected travel time corresponding to an increase of one mile in the distance traveled when the number of deliveries is held constant.

- $b_2 = +0.9234$ (for $x_2$)

  +0.9234 is the estimated change (increase) in the expected travel time corresponding to an increase of one delivery when the number of miles traveled is held constant.

## Multiple Coefficient of Determination (1 of 3)

- Relationship Among SST, SSR, SSE

|                        | SST                             | SSR                    | SSE                        |
|------------------------|---------------------------------|------------------------|----------------------------|
| Formula                | $\sum (y_i - \bar{y})^2$    | $\sum (\hat{y}_i - \bar{y})^2$ | $\sum (y_i - \hat{y}_i)^2$ |
| Degrees of Freedom      | $n-1$                       | $p$                | $n-p-1$                |


$$
\sum (y_i - \bar{y})^2 = \sum (\hat{y_i} - \bar{y})^2 + \sum (y_i - \hat{y_i})^2
$$

Where:

- SST = Total sum of squares (total variation of the response)
- SSR = Sum of squares due to regression (explained by all predictors)
- SSE = Sum of squares due to error (unexplained variation of residuals)

## Partition of SST

XXX include table XXX


## Multiple Coefficient of Determination

**Example: Butler Trucking Company**

**ANOVA Table**

| ANOVA       | df | SS     | MS     | F       | Significance F |
|-------------|----|--------|--------|---------|----------------|
| Regression  | 2  | 21.6006| 10.8003| 32.8784 | 0.0003         |
| Residual    | 7  | 2.2994 | 0.3285 |         |                |
| **Total**   | 9  | 23.9   |        |         |                |

XXXX include image? XXX

## Multiple Coefficient of Determination 

**Example: Butler Trucking Company**

$$
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
$$

$$
R^2 = \frac{21.6006}{23.9} = 1 - \frac{2.2994}{23.9} = 90.38\%
$$

$$
R^2 = 1 - \frac{SSE / (n - 1)}{SST / (n - 1)} = 1 - \frac{\text{Residual variance}}{\text{Response variance}}
$$


## Remarks on Multiple Coefficient of Determination

- Adding independent variables, even ones that are not statistically significant, will reduce the prediction errors, thus the SSE will become smaller.

- Because SST = SSR + SSE is fixed, SSR will become larger and hence, $R^2 = SSR/SST$ will always increase.

- But, adding independent variables, the model will become more complex (a larger $p$).



## Adjusted Multiple Coefficient of Determination 

The **adjusted multiple coefficient of determination** ($R^2_a$) takes into account the following factors:

- The number of independent variables in the model (p, dimension or complexity)
- The $R^2$ (Adequacy)
- The sample size (n, available information)

## Adjusted Multiple Coefficient of Determination 

Example: Butler Trucking Company

$$
R^2 = 1 - \frac{SSE}{SST} = 90.38\%
$$

$$
R^2_a = 1 - \frac{SSE / (n - p - 1)}{SST / (n - 1)} = 87.63\%
$$


## Adjusted Multiple Coefficient of Determination 

$$
R^2_a = 1 - \frac{SSE / (n - p - 1)}{SST / (n - 1)} = 1 - \frac{MSE}{SST / (n - 1)}
$$

- A smaller S² gives a higher $R^2_a$.
- Bringing in a new independent variable will increase $R^2_a$ only when it decreases the estimated error variance (S² or MSE).
- Hence, $R^2_a$ is a good criterion for model building or model selection.

## Testing for Significance

- In simple linear regression, the F and t tests provide the same conclusion:
  - They are equivalent tests: Squared t-value = F-value.
  - They have the same p-value.
- In multiple regression, the F and t tests have different purposes and are not equivalent.

## Testing for Overall Significance: $F$ Test 

- The $F$ Test is used to determine whether a significant linear relationship exists between the dependent variable and the set of all the independent variables, by testing their population slopes ($\beta_1, \dots, \beta_p$) together as a group.

- The $F$ Test is referred to as the test for overall significance.

## Testing for Overall Significance: $F$ Test 

**Hypotheses**

$$
H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0 \quad (\text{No linear relationship})
$$
$$
H_a: \text{One or more of the slopes is not equal to zero}
$$

**Test Statistics**

$$
F = \frac{MSR}{MSE}
$$

**Rejection Rule**

Reject $H_0$ if $F \geq F_\alpha$ or $p \leq \alpha$.

where $F_\alpha$ is based on an $F$ distribution with $p$ d.f. (numerator) and $n - p - 1$ d.f. (denominator).

## Testing for Overall Significance: $F$ Test 

**Example: Butler Trucking Company**

**Hypotheses:**

$$
H_0: \beta_1 = \beta_2 = 0
$$
$$
H_a: \text{One or both slopes are not equal to zero}
$$

**Rejection Rule**

For $\alpha = 0.01$ and degrees of freedom = 2 and 7, $F_{\alpha} = 9.55$.

Reject $H_0$ if $F \geq 9.55$ or $p \leq .01$.


## Testing for Overall Significance: $F$ Test (4 of 5)

**Test Statistics**

$$
F = \frac{MSR}{MSE} = \frac{10.8003}{0.3285} = 32.8784
$$

**Conclusion:**

We reject $H_0$ because $F = 32.8784 > 9.55$.

We have significant statistical evidence to conclude that at least one of the independent variables is useful in explaining the variation in the total travel time, or for predicting the total travel time. The significance level is 1%.


## Testing for Overall Significance: $F$ Test (5 of 5)

**ANOVA Output**

| Source      | df  | SS       | MS       | F       | Significance F |
|-------------|-----|----------|----------|---------|----------------|
| Regression  | 2   | 21.6006  | 10.8003  | 32.8784 | 0.0003         |
| Residual    | 7   | 2.2994   | 0.3285   |         |                |
| Total       | 9   | 23.9     |          |         |                |

$$
p = 1 - F.DIST(32.8784, 2, 7, TRUE) \approx 0.0003
$$




## Testing for Individual Significance: t Test (1 of 5)

- If the $F$ test shows an overall significance, a separate $t$ test is conducted for each of the independent variables in the model.

- The $t$ test is used to determine whether each of the individual independent variables is significant **in the presence of all other independent variables in the model**.

    - It tests the **additional contribution** in reducing the variation of the unexplained error (or in increasing the explained variation).

- We refer to each of these t tests as a test for individual significance.


## Testing for Individual Significance: t Test (2 of 5)

**Hypotheses**

$$
H_0: \beta_i = 0 \quad (x_i \text{ is not significant in the presence of all other } x's)
$$

$$
H_a: \beta_i \neq 0
$$

**Test Statistic**

$$
t = \frac{b_i - 0}{s_{b_i}} \quad (s_{b_i} \text{ is the standard error of } b_i)
$$

**Rejection Rule**

Reject $H_0$ if $t \leq -t_{\alpha / 2}$ or $t \geq t_{\alpha / 2}$ or if $p$-value $\leq \alpha$.

where $t_{\alpha/2}$ is based on a $t$-distribution with $n - p - 1$ degrees of freedom.


XXXXXXXXXXXXXXXXXXXXXXXXXXXXX

XXX Parei aqui embaixo

## Testing for Individual Significance: t Test (3 of 5)

**Example: Butler Trucking Company**

- $H_0: \beta_i = 0$
- $H_a: \beta_i \neq 0 \quad \text{(Two-tailed)}$

For $\alpha = 0.01$ and $df = 7$:

$$
t_{0.005} = 3.499
$$

Reject $H_0$ if $p$-value $\leq .01$, or if $t \leq -3.499$ or $t \geq 3.499$.
```

---

### Slide 7: Testing for Individual Significance: t Test (4 of 5)

```markdown
---
title: "Testing for Individual Significance: t Test (4 of 5)"
---

# Testing for Individual Significance: t Test (4 of 5)

**Test Statistics**

$$
t = \frac{b_1 - 0}{s_{b_1}} = \frac{0.0611 - 0}{0.0099} = 6.1717, \quad \text{for } x_1
$$

$$
t = \frac{b_2 - 0}{s_{b_2}} = \frac{0.9234 - 0}{0.2211} = 4.1764, \quad \text{for } x_2
$$

**Conclusion**

Reject each of $H_0: \beta_1 = 0$ and $H_0: \beta_2 = 0$. Each independent variable is statistically significant **in the presence of the other**.
```

---

### Slide 8: Testing for Individual Significance: t Test (5 of 5)

```markdown
---
title: "Testing for Individual Significance: t Test (5 of 5)"
---

# Testing for Individual Significance: t Test (5 of 5)

**Example: Butler Trucking Company**

- Regression Table

| Coefficients | Standard Error | t Stat  | P-value  |
|--------------|----------------|---------|----------|
| Intercept    | -0.8687        | 0.9515  | -0.9129  | 0.3916   |
| Miles        | 0.0611         | 0.0099  | 6.1824   | 0.0005   |
| Deliveries   | 0.9234         | 0.2211  | 4.1763   | 0.0042   |

- **Which of the two independent variables is more significant?**
```

---

### Slide 9: Multicollinearity (1 of 2)

```markdown
---
title: "Multicollinearity (1 of 2)"
---

# Multicollinearity (1 of 2)

- The term **multicollinearity** refers to the correlation among the independent variables.
- It is important to compute pairwise correlations in a multiple regression analysis.
- When independent variables are highly correlated (e.g., $|r| > 0.7$), it is difficult to determine the separate/marginal effect of each independent variable on the dependent variable.
- Effort should be made to avoid including independent variables that are highly correlated.
```

---

### Slide 10: Multicollinearity (2 of 2)

```markdown
---
title: "Multicollinearity (2 of 2)"
---

# Multicollinearity (2 of 2)

- Coefficients do not measure marginal effects of the predictors (combined effects may be measured).
- Coefficient estimates (b’s) are **unstable** (with inflated standard errors—see Variance Inflation Factor, VIF).
- **Non-significant** test results for important predictor variables.
- Estimated regression coefficients may have unexpected signs, opposite of theoretical expectations.
- **Large changes** in coefficients when a variable is added or deleted.
- **Wide confidence intervals** for important predictor variables.
```

---

Here is the Quarto code for the slides based on the images you uploaded:

```markdown
---
title: "Categorical Independent Variables in Regression"
format: revealjs
---

## Prediction

- The procedures for estimating the mean value of $y$ and predicting an individual value of $y$ in multiple regression are similar to those in simple regression. The interpretations of CI (Confidence Interval) and PI (Prediction Interval) are also similar.
- We substitute the given values of $x_1, x_2, ..., x_p$ into the estimated regression equation and use the corresponding value of $\hat{y}$ as the point estimate.
- The formulas required to develop interval estimates, CI and PI, are beyond the scope of the textbook. Software packages for multiple regression will often provide these interval estimates.

---

## Categorical Independent Variables (1 of 9)

- In many situations, we need to work with categorical independent variables such as gender (an independent variable with 2 categories: Male, Female), method of payment (an independent variable with 3 categories: Cash, Check, Credit card), etc.
- Need to code categorical independent variables. For example, if $x_2$ is a gender variable, let $x_2 = 1$ for males and $x_2 = 0$ for females.
- In this case, $x_2$ is called a dummy or indicator variable, and the category with a 0 (i.e., female) is called the reference category or level.

---

## Categorical Independent Variables (2 of 9)

- Gender: F, M
- Job Type: Lawyer, Salesperson, Educator, Government (for salary)
- Tool Type: A, B, C (for yields)
- Region: NE, NW, SE, SW (for sales)
- Season (for sales)
- Training Method: Online, Hybrid, In-person (for learning outcome)
- Position: Manager, AVP, Senior VP, Chair (for retirement age)
- Advertising Media: Newspaper, Magazine, Spot TV (for sales)
- Smoker: Yes, No (for health risk)
- Method of Payment: Cash, Check, Credit Card (for credit rating or loan approvals)

---

## Categorical Independent Variables (3 of 9)

**Example: Johnson Filtration, Inc.**

Managers of Johnson Filtration, Inc. want to predict the repair time necessary for processing its maintenance requests. Repair time is believed to be related to two factors, the number of months since last service and the type of repair problem (mechanical or electrical). Data for a sample of 10 service calls are reported in the table below.

---

## Categorical Independent Variables (4 of 9)

| Service Call | Months since last service | Type of repair | Repair time (hours) |
|--------------|---------------------------|----------------|---------------------|
| 1            | 2                         | 1              | 2.9                 |
| 2            | 6                         | 0              | 3.0                 |
| 3            | 8                         | 1              | 4.8                 |
| 4            | 3                         | 0              | 1.8                 |
| 5            | 2                         | 1              | 2.9                 |
| 6            | 7                         | 1              | 4.9                 |
| 7            | 9                         | 0              | 4.2                 |
| 8            | 8                         | 0              | 4.8                 |
| 9            | 4                         | 1              | 4.4                 |
| 10           | 6                         | 1              | 4.5                 |

0 = Mechanical (reference)  
1 = Electrical

---

## Categorical Independent Variables (5 of 9)

**Example: Johnson Filtration, Inc.**

Regression model:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon
$$

Where:  
$y$ = Repair time in hours  
$x_1$ = Number of months since last maintenance service  
$x_2$ = 0 if type of repair is mechanical, 1 if the type of repair is electrical ( $x_2$ is a dummy variable)

---

## Categorical Independent Variables (6 of 9)

**Example: Johnson Filtration, Inc.**

ANOVA Output:

| df          | SS     | MS     | F      | Significance F |
|-------------|--------|--------|--------|----------------|
| Regression  | 2      | 9.0009 | 4.5005 | 21.357 | 0.0010        |
| Residual    | 7      | 1.4751 | 0.2107 |        |                |
| Total       | 9      | 10.476 |        |        |                |

$$
R^2 = \frac{9.0009}{10.476} = 89.52\%
$$

$$
R_a^2 = 1 - (1 - 0.8952) \cdot \frac{10 - 1}{10 - 2 - 1} = 81.90\%
$$

This indicates the estimated regression equation does a good job of explaining the variability in repair times.

---

## Categorical Independent Variables (7 of 9)

- Interpretation of regression coefficient $\beta_2$ of dummy variable $x_2$:

$$
E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

For mechanical (reference):  
$$
E(y) = \beta_0 + \beta_1 x_1 + \beta_2 \cdot 0 = \beta_0 + \beta_1 x_1
$$

For electrical:  
$$
E(y) = \beta_0 + \beta_1 x_1 + \beta_2 \cdot 1 = \beta_0 + \beta_1 x_1 + \beta_2
$$

$\beta_2$ is the difference in the expected repair time between two types of repair **if the numbers of months since last maintenance service ($x_1$) are the same.**

---

## Categorical Independent Variables (8 of 9)

**Example: Johnson Filtration, Inc.**

| Coefficients | Standard Error | t Stat | P-value  |
|--------------|----------------|--------|----------|
| Intercept    | 0.9305         | 0.4670 | 1.9926   | 0.0866   |
| Months       | 0.3876         | 0.0626 | 6.1954   | 0.0004   |
| Type         | 1.2627         | 0.3141 | 4.0197   | 0.0051   |

Months since last service is significant at 1% significance level (in the presence of the second variable, or if we control for Type of problem).

Type of problem is also significant at 1% significance level if we control for the Months, and an electrical problem is predicted or estimated to take a longer time (by 1.1627) to repair than a mechanical one (the reference category).

---

## More Complex Categorical Variables (9 of 9)

- If a categorical variable has $k$ levels, only $k - 1$ dummy variables are required, with each dummy variable being coded as 0 or 1.

**Example**: Quarterly sales (dependent variable) vs. Sales regions

| Region | DA | DB | DC |
|--------|----|----|----|
| A      | 1  | 0  | 0  |
| B      | 0  | 1  | 0  |
| C      | 0  | 0  | 1  |

Use only two (any two) dummy variables; i.e., two dummy variables are sufficient to differentiate the three regions.
```

---
title: "Multiple Regression Analysis Slides"
format: revealjs
---

## Assumptions About the Error Term $\epsilon$

- The error term $\epsilon$ is a random variable reflecting the deviation between the observed $y$ value and the expected value of $y$ given by $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p$.
- $\epsilon$ is a normally distributed random variable with **mean of 0**.
- The variance of $\epsilon$, denoted by $\sigma^2$, is the **same** for all values of the independent variables.
- The values of $\epsilon$ are **independent**.

---

## Residual Analysis

- For simple linear regression, the residual plot against $\hat{y}$ and the residual plot against $x$ provide the same information.
- In multiple regression analysis, it is preferable to use the residual plot against $\hat{y}$ to determine if the model assumptions are satisfied.

---

## Standardized Residuals (1 of 3)

- Standardized residuals are frequently used in residual plots for purposes of:
  - Identifying outliers (typically, standardized residuals $< -2$ or $> +2$)
  - Providing insight about the assumption that the error term $\epsilon$ has a normal distribution, using the Empirical Rule.
- The computation of standardized residuals in multiple regression analysis is too complex to be done by hand.
  - Excel’s Regression tool can be used.
- Normality of residuals can be checked by the normal probability plot.

---

## Standardized Residuals (2 of 3)

**Example: Butler Trucking Company: Residual output**

| Observation | Predicted Time | Residuals | Standard Residuals |
|-------------|----------------|-----------|--------------------|
| 1           | 8.9385         | 0.3615    | 0.7153             |
| 2           | 4.9583         | -0.1583   | -0.3132            |
| ...         | ...            | ...       | ...                |

---

## Standardized Residual Plot Against $\hat{y}$ (3 of 3)

**Example: Butler Trucking Company**

![Standardized Residual Plot Against Predicted Time](path/to/plot-image.png)

---

## Adding or Removing Independent Variables?

- If adding a predictor to a model can lower the estimated error variance $S^2$, then adding it will increase Adjusted $R^2$.
- **Definition**: A predictor is said to be an **ineffective predictor** if its $|T\text{-value}|$
  - $\frac{|b-0|}{se(b)} < 1$ (coefficient estimate $b$ is within one standard deviation from 0), or its two-sided p-value $> 0.32$.
  - Adding an ineffective predictor will decrease Adjusted $R^2$.
  - Removing an ineffective predictor will increase Adjusted $R^2$.
  - We will be penalized for including ineffective predictors.



XXXXX

# Summary {background-color="#cfb991"}

## Summary

::: nonincremental
Some key takeaways from this session:

-   XXXX
  
    -   XXXX


:::

# Thank you! {background-color="#cfb991"}
